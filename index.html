<style>
body{
margin:0 auto;
}
</style><html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=n6RTCDcIPWSE8UNBa4k-DPok5wI36VK7fT3eoiKftDm4f9NsDRK2QhXrY8aUq2X0');ol{margin:0;padding:0}table td,table th{padding:0}.c12{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:84.8pt;border-top-color:#000000;border-bottom-style:solid}.c27{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:141pt;border-top-color:#000000;border-bottom-style:solid}.c29{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:144pt;border-top-color:#000000;border-bottom-style:solid}.c34{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:351pt;border-top-color:#000000;border-bottom-style:solid}.c17{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:121.5pt;border-top-color:#000000;border-bottom-style:solid}.c18{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:354pt;border-top-color:#000000;border-bottom-style:solid}.c25{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:125.2pt;border-top-color:#000000;border-bottom-style:solid}.c16{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:88.5pt;border-top-color:#000000;border-bottom-style:solid}.c1{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Old Standard TT";font-style:italic}.c19{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Old Standard TT";font-style:normal}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Old Standard TT";font-style:italic}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c28{color:#000000;font-weight:700;vertical-align:baseline;font-size:14pt;font-family:"Old Standard TT";font-style:normal}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c36{color:#000000;text-decoration:none;vertical-align:baseline;font-size:13pt;font-style:normal}.c14{color:#000000;text-decoration:none;vertical-align:baseline;font-size:12pt;font-style:normal}.c32{margin-left:72.8pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c24{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c33{margin-left:71.2pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c13{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c21{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c5{font-weight:400;font-family:"Old Standard TT"}.c4{color:inherit;text-decoration:inherit}.c30{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c23{text-indent:36pt}.c9{height:24.6pt}.c26{margin-left:36pt}.c20{background-color:#ffffff}.c22{vertical-align:super}.c15{height:11pt}.c11{font-style:italic}.c10{height:0pt}.c35{height:21pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c20 c30 doc-content"><p class="c8"><span class="c21 c28">CS 4641 Final Report: A.N.I.M.E</span></p><p class="c8"><span class="c5">by Abhay Sheshadri, Nick Chapman, Abdul Sayed, and Kevin Xiao</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c5 c11 c21">Introduction</span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c7 c23"><span class="c0">Our idea is to train a model to colorize and upscale low-resolution black-and-white anime portrait images to high-resolution colored ones. &nbsp;The purpose of this is to determine whether or not low-resolution b&amp;w images are a good compressed representation of the original image. &nbsp;By being able to sufficiently reconstruct low-quality images back into their source, the model will have effectively learned a domain-specific compression algorithm that can yield far greater metrics than general-purpose ones. &nbsp;There is also a practical reason to look into this - manga panels are black and white and their scans are often in a low resolution. &nbsp;Hopefully, studying this problem can make progress toward building systems that can colorize and upscale manga scans. &nbsp;There is potentially a huge market for this kind of product, especially amongst western consumers of Japanese media.</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c1">Background</span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Several published works have made progress in this area although they were primarily trained to reconstruct human faces from semantic masks or sketches. Generative adversarial networks (GAN) are one type of network that can model a distribution of images well. GANs use a limited form of supervision called self-supervision to operate. They work by having two different neural networks (NN) compete with one another - the primary NN&rsquo;s gains are the secondary NN&rsquo;s loss. The primary NN is known as the generator and the secondary NN is known as the discriminator and they work in tandem to both generate images and judge their accuracy.</span></p><p class="c6"><span class="c0"></span></p><p class="c7 c23"><span class="c0">StyleGAN [3], a type of generative adversarial network, has been shown in the past to be able to learn the distribution of human faces. It is able to consistently produce very high-resolution detailed images and can be used as a part of larger architectures for image-to-image tasks. &nbsp;Pixel2Style2Pixel [4] was one of the first works to use a StyleGAN architecture to solve image-to-image problems. They train a StyleGAN to model the target distribution and then train an encoder network to map images from the source distribution to the latent space of the StyleGAN. &nbsp;This makes it possible to convert source images to latent vectors, and then convert latent vectors to target images using StyleGAN. &nbsp;This approach is superior to methods that have traditionally been used for image-to-image tasks such as conditional GANs and U-nets. &nbsp;</span></p><p class="c6 c23"><span class="c0"></span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Other works continue this line of research. &nbsp;EditGAN [1] essentially trains multiple different GANs for both the target and source distribution such that they all share the same latent space. &nbsp;They then optimize a latent vector to reconstruct the source image, which they can then use to get the target image from a different GAN since the latent space is shared. PULSE [2] uses StyleGAN to upscale images. &nbsp;They find a way of effectively exploring the latent space of the GAN to find an image that corresponds to the downscaled one. </span></p><p class="c6"><span class="c0"></span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c1">Problem Definition</span></p><p class="c6"><span class="c2"></span></p><p class="c7 c23"><span class="c0">Our problem is an image-to-image task. &nbsp;More specifically, it can be considered a super-resolution task as we have to convert low-resolution images to ones that are 64 times larger (32x32 to 256x256). &nbsp;The super-resolution problems that are typically studied are usually not as extreme; they typically try to increase the number of pixels by 4-16 times. &nbsp;Our problem is also a colorization task as it involves reproducing the red, green, and blue channels from the single greyscale channel.</span></p><p class="c6 c23"><span class="c0"></span></p><p class="c7 c23"><span class="c0">Since downscaling and de-coloring an image is a lossy algorithm, we need to model the distribution of anime portraits with a GAN in order to reconstruct images from partial information. &nbsp;We also want to generate a spectrum of valid images, as there may be several possible images that may correspond to the low-quality input.</span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c1">Data Collection</span></p><p class="c7 c23"><span class="c0 c20">We intend to achieve this by using the anime-faces dataset by Gwern, which was created from scraping various image boards where artists post their works. &nbsp;An automated face-cropping tool was developed and used to extract just the areas around the faces in the images.</span></p><p class="c6"><span class="c0 c20"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.13px; height: 283.13px;"><img alt="" src="images/image2.png" style="width: 300.13px; height: 283.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c5 c20 c11">Samples from the dataset</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c5">The dataset can be downloaded from </span><span class="c13 c5"><a class="c4" href="https://www.google.com/url?q=https://www.gwern.net/Faces%23portrait-dataset&amp;sa=D&amp;source=editors&amp;ust=1670392192602646&amp;usg=AOvVaw2NPKIfD9OG8wtQKp1GdJse">https://www.gwern.net/Faces#portrait-dataset</a></span><span class="c5">&nbsp;</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c0">Image_Size: 256x256 pixels</span></p><p class="c7"><span class="c0"># Of Images: 226,037</span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c1">Methods</span></p><p class="c6 c23"><span class="c0"></span></p><p class="c7 c23"><span class="c0">Our approach borrows from Pixel2Style2Pixel - we will try to find a latent code that corresponds to the source image. &nbsp;To do this, we have a two-stage approach. &nbsp;The first stage involves training a generative model to learn and reproduce the distribution of anime portrait images. &nbsp;The generative model will be to produce a high-quality anime portrait image given a latent code, sampled from a normal distribution. &nbsp;In the second stage, we will train an auxiliary model to reconstruct the latent codes from the generated images, essentially inverting the model learned during the previous step. &nbsp;For our particular problem, we must first degrade the generated image (downscaling and removing color) before putting it through the auxiliary dataset. </span></p><p class="c6 c23"><span class="c0"></span></p><p class="c7 c23"><span class="c0">For the first stage, we train a StyleGAN3 model on a dataset of anime portrait images. &nbsp;We initialize the model with the provided weights from training on the FFHQ dataset (high-quality human faces). &nbsp;Since the structure of anime faces is similar enough to human faces, this would drastically reduce the amount of training required to produce good results. &nbsp;We remove some of the features of the model that are not necessary for our application; we get rid of rotational equivariance, decrease the maximum number of channels at any layer to 16384, and decrease the number of layers to 16. &nbsp;Most of these choices were made to speed up the training of the model; we do not believe that this would have a major impact on anime face generation as the dataset lacks high-frequency details and does not have many rotated faces. &nbsp;We use the existing PyTorch implementation provided by NVIDIA and resume training from their provided FFHQ checkpoint.</span></p><p class="c6 c23"><span class="c0"></span></p><p class="c8 c23"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 297.30px; height: 282.76px;"><img alt="" src="images/image1.png" style="width: 297.30px; height: 282.76px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c23"><span class="c2">StyleGAN3 Architecture [3]</span></p><p class="c8 c23 c15"><span class="c2"></span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We train an auxiliary model for the second stage to reconstruct the latent code from the degraded generated images. &nbsp;For the architecture of the model, we choose to use a Residual Network [5] which we implemented in PyTorch. &nbsp;The model takes in the degraded image and outputs a 512-dimensional vector which serves as the prediction of the latent code. &nbsp;Each ResNet block consists of two convolutional layers and batch normalization layers with LeakyReLU activations in the middle. &nbsp;We also add Squeeze-Excitation layers between blocks of the model to improve the expressibility of the model. &nbsp;Each model has an input convolution at the start to increase the number of channels to 64 and a fully-connected layer at the end to make sure the output has the correct dimensionality. &nbsp;We use two different configurations in our experiments: ResNet-50 and ResNet-18Two</span></p><p class="c6"><span class="c0"></span></p><a id="t.bd89408816b71184ea05d94775762f510c8eaa25"></a><a id="t.0"></a><table class="c32"><tr class="c35"><td class="c34" colspan="3" rowspan="1"><p class="c3"><span class="c0">ResNet-18</span></p></td></tr><tr class="c10"><td class="c12" colspan="1" rowspan="1"><p class="c3"><span class="c0">Number of blocks</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c3"><span class="c0">Input Channels</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c3"><span class="c0">Output Channels</span></p></td></tr><tr class="c10"><td class="c12" colspan="1" rowspan="1"><p class="c3"><span class="c0">2</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c3"><span class="c0">64</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c3"><span class="c0">64</span></p></td></tr><tr class="c10"><td class="c12" colspan="1" rowspan="1"><p class="c3"><span class="c0">2</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c3"><span class="c0">64</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c3"><span class="c0">128</span></p></td></tr><tr class="c10"><td class="c12" colspan="1" rowspan="1"><p class="c3"><span class="c0">2</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c3"><span class="c0">128</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c3"><span class="c0">256</span></p></td></tr><tr class="c10"><td class="c12" colspan="1" rowspan="1"><p class="c3"><span class="c0">2</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c3"><span class="c0">256</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c3"><span class="c0">512</span></p></td></tr></table><p class="c8 c15"><span class="c0"></span></p><a id="t.9dc2ce64841ed681e6e7ecd8f7f62521716c64e0"></a><a id="t.1"></a><table class="c33"><tr class="c9"><td class="c18" colspan="3" rowspan="1"><p class="c3"><span class="c0">ResNet-50</span></p></td></tr><tr class="c10"><td class="c16" colspan="1" rowspan="1"><p class="c3"><span class="c0">Number of blocks</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c0">Input Channels</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c3"><span class="c0">Output Channels</span></p></td></tr><tr class="c10"><td class="c16" colspan="1" rowspan="1"><p class="c3"><span class="c0">3</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c0">64</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c3"><span class="c0">64</span></p></td></tr><tr class="c10"><td class="c16" colspan="1" rowspan="1"><p class="c3"><span class="c0">4</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c0">64</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c3"><span class="c0">128</span></p></td></tr><tr class="c10"><td class="c16" colspan="1" rowspan="1"><p class="c3"><span class="c0">14</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c0">128</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c3"><span class="c0">256</span></p></td></tr><tr class="c10"><td class="c16" colspan="1" rowspan="1"><p class="c3"><span class="c0">3</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c0">256</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c3"><span class="c0">512</span></p></td></tr></table><p class="c8 c15"><span class="c0"></span></p><p class="c7"><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Due to StyleGAN&rsquo;s structure, it naturally learns different embeddings for the structure and color components of the image. &nbsp;The latent codes for the first 8 layers of the model determine the structure of the character&rsquo;s face the latent codes for the final 8 layers determine the color of the image. &nbsp;Therefore, we only train the network to predict the first 8 latent codes and randomly sample new latent codes for the color. &nbsp;This means that the network does not try to infer anything about the color of the ground-truth image - it only cares about the general structural features of the face. &nbsp;We also operate in the W</span><span class="c5 c22">+</span><span class="c0">&nbsp;latent space of the model instead of the standard Z as it is more semantically meaningful and can express a wider range of faces. &nbsp;Given a StyleGAN generator G and a ResNet encoder E, we try to minimize</span></p><p class="c6"><span class="c0"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 352.50px; height: 31.88px;"><img alt="" src="images/image20.png" style="width: 352.50px; height: 31.88px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0">&nbsp;</span></p><p class="c7"><span class="c5">for a latent code </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 12.00px; height: 8.00px;"><img alt="" src="images/image8.png" style="width: 12.00px; height: 8.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0">. &nbsp;We call this function the &ldquo;MSE Latent Loss&rdquo; in our experiments. &nbsp;We freeze the parameters of G while training E as it is not necessary to update the parameters of the generator.</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c1">Results and Discussion</span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We trained a StyleGAN 3 model for 500 kilo-images with a batch size of 32, a learning rate of 0.002, and a gamma value of 2. &nbsp;To test how well the model performs, we randomly sample 10 latent vectors and generate images from them. &nbsp;We &ldquo;truncate&rdquo; the latent vectors so that they are closer to the average latent vector during training, to increase the quality of the images at the cost of some image diversity.</span></p><p class="c6"><span class="c0"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 541.50px; height: 107.91px;"><img alt="" src="images/image19.jpg" style="width: 541.50px; height: 107.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 540.50px; height: 108.27px;"><img alt="" src="images/image26.jpg" style="width: 540.50px; height: 108.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c2">The output of our StyleGAN model - newly generated never-before-seen anime faces</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also randomly sampled latent codes and interpolated them, creating animated videos of latent space walks.</span></p><p class="c6"><span class="c0"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 256.00px; height: 256.00px;"><img alt="" src="images/image10.gif" style="width: 256.00px; height: 256.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 256.00px; height: 256.00px;"><img alt="" src="images/image15.gif" style="width: 256.00px; height: 256.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c5 c11">Latent Space Walks</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The examples shown above were chosen without cherrypicking, demonstrating the consistent high quality of our GAN. &nbsp;The smoothness of the latent space walks demonstrates the richness of the learned latent space. &nbsp;We then trained a ResNet encoder to reconstruct the generated images (discussed in detail in </span><span class="c5 c11">Methods</span><span class="c0">). &nbsp;Here are the initial results after training for 25 epochs with a learning rate of 0.001 and a batch size of 32. &nbsp;</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c0">&nbsp; &nbsp; &nbsp; GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 299.50px; height: 61.08px;"><img alt="" src="images/image14.png" style="width: 299.50px; height: 61.08px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5">&nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 294.50px; height: 58.90px;"><img alt="" src="images/image25.png" style="width: 294.50px; height: 58.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 297.02px; height: 59.50px;"><img alt="" src="images/image27.png" style="width: 297.02px; height: 59.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5 c11">&nbsp; &nbsp; &nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 293.50px; height: 58.50px;"><img alt="" src="images/image16.png" style="width: 293.50px; height: 58.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 295.50px; height: 58.74px;"><img alt="" src="images/image12.png" style="width: 295.50px; height: 58.74px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5 c11">&nbsp; &nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 290.97px; height: 57.80px;"><img alt="" src="images/image11.png" style="width: 290.97px; height: 57.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c2">Initial results</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c0">Here is the loss over training (each epoch is 100 newly generated batches of images) :</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 378.00px; height: 283.19px;"><img alt="" src="images/image22.png" style="width: 378.00px; height: 283.19px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c2">Loss over time</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Upon further exploration, we figured out that smaller ResNet architectures work just as well and can be trained for longer. &nbsp;We trained ResNet 18 on different resolutions for 50 epochs and recorded the performance over training:</span></p><p class="c6"><span class="c0"></span></p><p class="c7 c23 c26"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;32x32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 64x64</span></p><p class="c7"><span class="c5">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 289.43px; height: 217.33px;"><img alt="" src="images/image13.png" style="width: 289.43px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5">&nbsp; &nbsp; &nbsp; &nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.79px; height: 215.30px;"><img alt="" src="images/image6.png" style="width: 288.79px; height: 215.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c2"></span></p><p class="c7 c23 c26"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; 128x128 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 256x256</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 310.50px; height: 230.88px;"><img alt="" src="images/image21.png" style="width: 310.50px; height: 230.88px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 303.50px; height: 228.61px;"><img alt="" src="images/image17.png" style="width: 303.50px; height: 228.61px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c2">Training plots for different resolutions on ResNet18</span></p><p class="c8 c15"><span class="c2"></span></p><p class="c8"><span class="c0">32 x 32</span></p><p class="c7"><span class="c5">&nbsp; &nbsp; &nbsp;GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 291.50px; height: 58.30px;"><img alt="" src="images/image4.jpg" style="width: 291.50px; height: 58.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5 c11">&nbsp; &nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 294.01px; height: 58.80px;"><img alt="" src="images/image18.jpg" style="width: 294.01px; height: 58.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c2"></span></p><p class="c8"><span class="c0">64 x 64</span></p><p class="c7"><span class="c5">&nbsp; &nbsp; &nbsp;GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 286.50px; height: 56.91px;"><img alt="" src="images/image7.jpg" style="width: 286.50px; height: 56.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5 c11">&nbsp; &nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 284.55px; height: 57.52px;"><img alt="" src="images/image3.jpg" style="width: 284.55px; height: 57.52px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c2"></span></p><p class="c8"><span class="c0">128 x 128</span></p><p class="c7"><span class="c5">&nbsp; &nbsp; &nbsp; GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 297.02px; height: 59.50px;"><img alt="" src="images/image24.jpg" style="width: 297.02px; height: 59.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5 c11">&nbsp; &nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 287.50px; height: 59.08px;"><img alt="" src="images/image23.jpg" style="width: 287.50px; height: 59.08px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c1"></span></p><p class="c8"><span class="c0">256 x 256</span></p><p class="c7"><span class="c5">&nbsp; &nbsp; &nbsp; GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;GT &nbsp; &nbsp;| &nbsp; Input &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; Reconstruction &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 297.50px; height: 59.50px;"><img alt="" src="images/image5.jpg" style="width: 297.50px; height: 59.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5 c11">&nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 296.86px; height: 59.57px;"><img alt="" src="images/image9.jpg" style="width: 296.86px; height: 59.57px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c5 c11">Examples of different resolutions on ResNet18 </span></p><p class="c6"><span class="c1"></span></p><p class="c7"><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We discovered a ResNet-18 network was able to sufficiently recover details about the facial structure, head orientation, hair length, and headwear from a 32 x 32 images. &nbsp;Increasing the size of the encoder or the resolution of the input image did not make learning easier; in fact, both of these things decreased the performance of the model and caused the MSE Latent Loss to increase. &nbsp;Convergence was much slower at higher resolutions and bigger models as shown in the training graphs. &nbsp;The full-resolution images caused the initial loss to be </span><span class="c5 c11">over 40</span><span class="c0">&nbsp;and finally ended with a higher loss (0.275) than the lower resolutions. &nbsp;Using a ResNet-50 made training quite unstable and didn&rsquo;t result in a much lower loss. &nbsp;The lowest loss was achieved by the ResNet-18 encoder on 64x64 images (0.248), but this wasn&rsquo;t much better than the loss of the ResNet-18 encoder on 32x32 images (0.250). &nbsp;This means that not much information was gained by upping the number of pixels from 32 x 32 to 64 x 64 and increasing the resolution beyond that had a negative effect.</span></p><p class="c6"><span class="c0"></span></p><p class="c7"><span class="c1">Conclusion</span></p><p class="c7"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;StyleGAN was able to produce a diverse range of high-quality images - &nbsp;however, the distribution might be slightly different from the dataset. &nbsp;This might happen since mode collapse is incentivized by the loss function of the GAN. &nbsp;Since our encoder was only trained on synthetic images, it may not generalize well to real images. &nbsp;The problem of failing to generalize over distribution shifts is still currently unsolved meaning that the model may not perform as well on real images.</span></p><p class="c6"><span class="c5 c19"></span></p><p class="c7"><span class="c1">Contribution Table</span></p><p class="c7"><span class="c5 c13"><a class="c4" href="https://www.google.com/url?q=https://gtvault.sharepoint.com/:x:/s/MLA.N.I.M.EProject/EZ1iOfVVKJxIiwB5AMjXszoBMY0VTNDXe1CdcyG_kcXUYA?e%3D1Q5LLx&amp;sa=D&amp;source=editors&amp;ust=1670392192620971&amp;usg=AOvVaw0jqzQJ-gLR_lZY-7urCnAD">https://gtvault.sharepoint.com/:x:/s/MLA.N.I.M.EProject/EZ1iOfVVKJxIiwB5AMjXszoBMY0VTNDXe1CdcyG_kcXUYA?e=1Q5LLx</a></span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c1">GANTT</span></p><p class="c7"><span class="c13 c5 c11"><a class="c4" href="https://www.google.com/url?q=https://1drv.ms/x/s!Aoim_xmzhNr9hYsUe4S9d_4HKS_qeA?e%3DmIgBO3&amp;sa=D&amp;source=editors&amp;ust=1670392192621761&amp;usg=AOvVaw1GmsjARo7r0Bal1mMKKVWp">https://1drv.ms/x/s!Aoim_xmzhNr9hYsUe4S9d_4HKS_qeA?e=mIgBO3</a></span><span class="c2">&nbsp;</span></p><p class="c6"><span class="c2"></span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c1">Citations</span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c2">[1] Ling, H., Kreis, K., Li, D., Kim, S., Torralba, A., &amp; Fidler, S.. (2021). EditGAN: High-Precision Semantic Image Editing.</span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c2">[2] Menon, S., Damian, A., Hu, S., Ravi, N., &amp; Rudin, C.. (2020). PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models.</span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c2">[3] Tero Karras, et al. &quot;Alias-Free Generative Adversarial Networks.&quot; Proc. NeurIPS. 2021.</span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c2">[4] Richardson, E., Alaluf, Y., Patashnik, O., Nitzan, Y., Azar, Y., Shapiro, S., &amp; Cohen-Or, D.. (2020). Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation.</span></p><p class="c6"><span class="c2"></span></p><p class="c7"><span class="c2">[5] He, K., Zhang, X., Ren, S., &amp; Sun, J.. (2015). Deep Residual Learning for Image Recognition.</span></p><p class="c6"><span class="c2"></span></p><p class="c6"><span class="c2"></span></p><p class="c6"><span class="c2"></span></p><div><p class="c6"><span class="c31"></span></p></div></body></html>